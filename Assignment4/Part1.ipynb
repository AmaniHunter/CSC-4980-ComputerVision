{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run this code in YOLOv3-Object-Detection-with-OpenCV\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2 as cv\n",
    "import subprocess\n",
    "from yolo_utils import infer_image, show_image\n",
    "import utils\n",
    "import depthai as dai\n",
    "\n",
    "def draw_text(img, text,\n",
    "          font=cv.FONT_HERSHEY_PLAIN,\n",
    "          pos=(0, 0),\n",
    "          font_scale=3,\n",
    "          font_thickness=2,\n",
    "          text_color=(0, 255, 0),\n",
    "          text_color_bg=(0, 0, 0)\n",
    "          ):\n",
    "\n",
    "    x, y = pos\n",
    "    text_size, _ = cv.getTextSize(text, font, font_scale, font_thickness)\n",
    "    text_w, text_h = text_size\n",
    "    cv.rectangle(img, pos, (x + text_w, y + text_h), text_color_bg, -1)\n",
    "    cv.putText(img, text, (x, y + text_h + font_scale - 1), font, font_scale, text_color, font_thickness)\n",
    "\n",
    "    return text_size\n",
    "\n",
    "scale = 3\n",
    "wP = 210 *scale\n",
    "hP= 297 *scale\n",
    "width = 800\n",
    "height = 600\n",
    "pipeline = dai.Pipeline()\n",
    "camRgb = pipeline.createColorCamera()\n",
    "# cam.initialControl.setManualFocus(130)\n",
    "xoutVideo = pipeline.createXLinkOut()\n",
    "xoutVideo.setStreamName(\"video\")\n",
    "xoutVideo.input.setBlocking(False)\n",
    "xoutVideo.input.setQueueSize(1)\n",
    "\n",
    "# Properties\n",
    "camRgb.setPreviewSize(width, height)\n",
    "camRgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.RGB)\n",
    "\n",
    "# Linking to preview stream\n",
    "camRgb.preview.link(xoutVideo.input)\n",
    "\n",
    "FLAGS = []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tparser = argparse.ArgumentParser()\n",
    "\n",
    "\tparser.add_argument('-m', '--model-path',\n",
    "\t\ttype=str,\n",
    "\t\tdefault='./yolov3-coco/',\n",
    "\t\thelp='The directory where the model weights and \\\n",
    "\t\t\t  configuration files are.')\n",
    "\n",
    "\tparser.add_argument('-w', '--weights',\n",
    "\t\ttype=str,\n",
    "\t\tdefault='./yolov3-coco/yolov3.weights',\n",
    "\t\thelp='Path to the file which contains the weights \\\n",
    "\t\t\t \tfor YOLOv3.')\n",
    "\n",
    "\tparser.add_argument('-cfg', '--config',\n",
    "\t\ttype=str,\n",
    "\t\tdefault='./yolov3-coco/yolov3.cfg',\n",
    "\t\thelp='Path to the configuration file for the YOLOv3 model.')\n",
    "\n",
    "\tparser.add_argument('-i', '--image-path',\n",
    "\t\ttype=str,\n",
    "\t\thelp='The path to the image file')\n",
    "\n",
    "\tparser.add_argument('-v', '--video-path',\n",
    "\t\ttype=str,\n",
    "\t\thelp='The path to the video file')\n",
    "\n",
    "\n",
    "\tparser.add_argument('-vo', '--video-output-path',\n",
    "\t\ttype=str,\n",
    "        default='./output.avi',\n",
    "\t\thelp='The path of the output video file')\n",
    "\n",
    "\tparser.add_argument('-l', '--labels',\n",
    "\t\ttype=str,\n",
    "\t\tdefault='./yolov3-coco/coco-labels',\n",
    "\t\thelp='Path to the file having the \\\n",
    "\t\t\t\t\tlabels in a new-line seperated way.')\n",
    "\n",
    "\tparser.add_argument('-c', '--confidence',\n",
    "\t\ttype=float,\n",
    "\t\tdefault=0.5,\n",
    "\t\thelp='The model will reject boundaries which has a \\\n",
    "\t\t\t\tprobabiity less than the confidence value. \\\n",
    "\t\t\t\tdefault: 0.5')\n",
    "\n",
    "\tparser.add_argument('-th', '--threshold',\n",
    "\t\ttype=float,\n",
    "\t\tdefault=0.3,\n",
    "\t\thelp='The threshold to use when applying the \\\n",
    "\t\t\t\tNon-Max Suppresion')\n",
    "\n",
    "\tparser.add_argument('--download-model',\n",
    "\t\ttype=bool,\n",
    "\t\tdefault=False,\n",
    "\t\thelp='Set to True, if the model weights and configurations \\\n",
    "\t\t\t\tare not present on your local machine.')\n",
    "\n",
    "\tparser.add_argument('-t', '--show-time',\n",
    "\t\ttype=bool,\n",
    "\t\tdefault=False,\n",
    "\t\thelp='Show the time taken to infer each image.')\n",
    "\n",
    "\tFLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "\t# Download the YOLOv3 models if needed\n",
    "\tif FLAGS.download_model:\n",
    "\t\tsubprocess.call(['./yolov3-coco/get_model.sh'])\n",
    "\n",
    "\t# Get the labels\n",
    "\tlabels = open(FLAGS.labels).read().strip().split('\\n')\n",
    "\n",
    "\t# Intializing colors to represent each label uniquely\n",
    "\tcolors = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n",
    "\n",
    "\t# Load the weights and configutation to form the pretrained YOLOv3 model\n",
    "\tnet = cv.dnn.readNetFromDarknet(FLAGS.config, FLAGS.weights)\n",
    "\n",
    "\t# Get the output layer names of the model\n",
    "\tlayer_names = net.getLayerNames()\n",
    "\tprint('names: ', layer_names)\n",
    "\tlayer_names = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\t#outputlayers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\t# If both image and video files are given then raise error\n",
    "\tcount = 0\n",
    "\n",
    "\tvid = cv.VideoCapture(0)\n",
    "\twith dai.Device(pipeline) as device:\n",
    "\t\tvideo = device.getOutputQueue(name=\"video\", maxSize=1, blocking=False)\n",
    "\n",
    "\t\twhile True:\n",
    "\t\t\tvideoIn = video.get()\n",
    "\t\t\t# Get BGR frame from NV12 encoded video frame to show with opencv\n",
    "\t\t\tframe = videoIn.getCvFrame()\n",
    "\t\t\tdimensions_frame = videoIn.getCvFrame()\n",
    "\t\t\theight, width = frame.shape[:2]\n",
    "\n",
    "\t\t\tif count == 0:\n",
    "\t\t\t\tframe, boxes, confidences, classids, idxs = infer_image(net, layer_names, \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\theight, width, frame, colors, labels, FLAGS)\n",
    "\t\t\t\tcount += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tframe, boxes, confidences, classids, idxs = infer_image(net, layer_names, \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\theight, width, frame, colors, labels, FLAGS,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tboxes, confidences, classids, idxs, infer=False)\n",
    "\t\t\t\tcount = (count + 1) % 6\n",
    "\n",
    "\t\t\t#cv.imshow('oakD', frame)\n",
    "\t\t\timgContours, conts = utils.getContours(dimensions_frame, minArea=50000, filter=4)\n",
    "\t\t\t#print('Img countours ', imgContours)\n",
    "\n",
    "\t\t\t#img = frame\n",
    "\t\t\tif len(conts) != 0:\n",
    "\t\t\t\tbiggest = conts[0][2]\n",
    "\t\t\t\tprint('biggest: ', biggest)\n",
    "\t\t\t\timgWarp = utils.warpImg(dimensions_frame, biggest, wP, hP)\n",
    "\t\t\t\timgContours2, conts2 = utils.getContours(imgWarp,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t minArea=2000, filter=4,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t cThr=[50, 50], draw=False)\n",
    "\n",
    "\t\t\t\tif len(conts) != 0:\n",
    "\t\t\t\t\tfor obj in conts2:\n",
    "\t\t\t\t\t\tcv.polylines(imgContours2, [obj[2]], True, (0, 255, 0), 2)\n",
    "\n",
    "\t\t\t\t\t\tnPoints = utils.reorder(obj[2])\n",
    "\t\t\t\t\t\tnW = round((utils.findDis(nPoints[0][0] // scale, nPoints[1][0] // scale) / 10), 1)\n",
    "\t\t\t\t\t\tnH = round((utils.findDis(nPoints[0][0] // scale, nPoints[2][0] // scale) / 10), 1)\n",
    "\t\t\t\t\t\tcv.arrowedLine(imgContours2, (nPoints[0][0][0], nPoints[0][0][1]),\n",
    "\t\t\t\t\t\t\t\t\t\t(nPoints[1][0][0], nPoints[1][0][1]),\n",
    "\t\t\t\t\t\t\t\t\t\t(255, 0, 255), 3, 8, 0, 0.05)\n",
    "\t\t\t\t\t\tprint('Coordinates: ', (nPoints[0][0][0], nPoints[0][0][1]))\n",
    "\t\t\t\t\t\tcv.arrowedLine(imgContours2, (nPoints[0][0][0], nPoints[0][0][1]),\n",
    "\t\t\t\t\t\t\t\t\t\t(nPoints[2][0][0], nPoints[2][0][1]),\n",
    "\t\t\t\t\t\t\t\t\t\t(255, 0, 255), 3, 8, 0, 0.05)\n",
    "\t\t\t\t\t\tx, y, w, h = obj[3]\n",
    "\t\t\t\t\t\t#cv\n",
    "\t\t\t\t\t\t#cv.putText(imgContours2, '{}cm'.format(nW), (x + 30, y - 10), cv.FONT_HERSHEY_COMPLEX_SMALL,\n",
    "\t\t\t\t\t\t\t\t#\t1.5,\n",
    "\t\t\t\t\t\t\t\t#\t(0, 0, 255), 2)\n",
    "\t\t\t\t\t\tdraw_text(imgContours2, '{}cm'.format(nW), font_scale=4, pos=(x + 30, y - 10),\n",
    "\t\t\t\t\t\t\t\t  text_color_bg=(255, 0, 0))\n",
    "\t\t\t\t\t\tprint('{}cm'.format(nW))\n",
    "\t\t\t\t\t\tprint('{}cm'.format(nH))\n",
    "\t\t\t\t\t\t#cv.putText(imgContours2, '{}cm'.format(nH), (x - 70, y + h // 2),\n",
    "\t\t\t\t\t\t\t\t\t#cv.FONT_HERSHEY_COMPLEX_SMALL,\n",
    "\t\t\t\t\t\t\t\t#\t1.5,\n",
    "\t\t\t\t\t\t\t\t#\t(0, 0, 255), 2)\n",
    "\t\t\t\t\t\tdraw_text(imgContours2, '{}cm'.format(nH), font_scale=4, pos=(x - 70, y + h // 2),\n",
    "\t\t\t\t\t\t\t\t  text_color_bg=(255, 0, 0))\n",
    "\n",
    "\t\t\t\t\t# cv2.namedWindow(\"measurement\", cv2.WND_PROP_FULLSCREEN)\n",
    "\t\t\t\t\t# cv2.setWindowProperty(\"measurement\", cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "\t\t\t\t\t# print('imgContours2', imgContours2)\n",
    "\t\t\t\t\t#m = cv.resize(imgContours2, (0, 0), None, 3, 3)\n",
    "\t\t\t\tcv.imshow('measurement', imgContours2)\n",
    "\t\t\t#for contour in imgContours:\n",
    "\t\t\t\t#cv.drawContours(frame, contour, -1, (0, 255, 0), 3)\n",
    "\t\t\t\t#cv.drawContours(frame, [contour], 0, (0, 0, 255), 2)\n",
    "\t\t\tcv.imshow('Original', frame)\n",
    "\t\t\tif cv.waitKey(1) == ord('q'):\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\n",
    "\n",
    "\tvid.release()\n",
    "\tcv.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}